# Titanic-Competition

My ML models using the Titanic data set.

Submissions have been made using the Xgboost model, the SVM model, and a stacked model that uses SVM, random forest, and Xgboost.  The solo Xgboost model has done the best.  The stacking procedure was a simple averaging of the binary classification results from all three models.  No extensive feature engineering or parameter tuning has been performed.  

There is also an R script with numerous other classification models, including Recursive Partitioning Trees, Conditional Inference Trees, KNN, Logistic Regression, and Naive Bayes Classifier.  This is just a test script to play around with the different models.  I haven't made a Kaggle submission with any of these scripts.
